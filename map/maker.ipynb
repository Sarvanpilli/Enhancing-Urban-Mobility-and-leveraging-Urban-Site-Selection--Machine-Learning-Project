{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with combined Category-Subcategory and weights saved to 'data_combined.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from CSV file\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Combine Category and Sub-Category into a new column\n",
    "df['Category-Subcategory'] = df['Category'] + '-' + df['Sub-Category']\n",
    "\n",
    "# Calculate the frequency of each Category-Subcategory\n",
    "frequency = df['Category-Subcategory'].value_counts()\n",
    "\n",
    "# Add a new column 'weights' based on the frequency\n",
    "df['weights'] = df['Category-Subcategory'].apply(lambda x: 1 / frequency[x])\n",
    "\n",
    "# Drop the original Category and Sub-Category columns\n",
    "df = df.drop(columns=['Category', 'Sub-Category'])\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('data_combined.csv', index=False)\n",
    "\n",
    "print(\"Data with combined Category-Subcategory and weights saved to 'data_combined.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Data_0_0.csv\n",
      "Saved Data_0_1.csv\n",
      "Saved Data_0_2.csv\n",
      "Saved Data_0_3.csv\n",
      "Saved Data_0_4.csv\n",
      "Saved Data_0_5.csv\n",
      "Saved Data_0_6.csv\n",
      "Saved Data_0_7.csv\n",
      "Saved Data_0_8.csv\n",
      "Saved Data_0_9.csv\n",
      "Saved Data_0_10.csv\n",
      "Saved Data_0_11.csv\n",
      "Saved Data_0_12.csv\n",
      "Saved Data_0_13.csv\n",
      "Saved Data_0_14.csv\n",
      "Saved Data_0_15.csv\n",
      "Saved Data_0_16.csv\n",
      "Saved Data_0_17.csv\n",
      "Saved Data_0_18.csv\n",
      "Saved Data_0_19.csv\n",
      "Saved Data_0_20.csv\n",
      "Saved Data_0_21.csv\n",
      "Saved Data_0_22.csv\n",
      "Saved Data_0_23.csv\n",
      "Saved Data_0_24.csv\n",
      "Saved Data_1_0.csv\n",
      "Saved Data_1_1.csv\n",
      "Saved Data_1_4.csv\n",
      "Saved Data_1_5.csv\n",
      "Saved Data_1_6.csv\n",
      "Saved Data_1_7.csv\n",
      "Saved Data_1_8.csv\n",
      "Saved Data_1_9.csv\n",
      "Saved Data_1_10.csv\n",
      "Saved Data_1_11.csv\n",
      "Saved Data_1_12.csv\n",
      "Saved Data_1_13.csv\n",
      "Saved Data_1_15.csv\n",
      "Saved Data_1_16.csv\n",
      "Saved Data_1_17.csv\n",
      "Saved Data_1_18.csv\n",
      "Saved Data_1_19.csv\n",
      "Saved Data_1_20.csv\n",
      "Saved Data_1_21.csv\n",
      "Saved Data_1_22.csv\n",
      "Saved Data_1_23.csv\n",
      "Saved Data_2_0.csv\n",
      "Saved Data_2_1.csv\n",
      "Saved Data_2_2.csv\n",
      "Saved Data_2_3.csv\n",
      "Saved Data_2_4.csv\n",
      "Saved Data_2_5.csv\n",
      "Saved Data_2_6.csv\n",
      "Saved Data_2_7.csv\n",
      "Saved Data_2_8.csv\n",
      "Saved Data_2_9.csv\n",
      "Saved Data_2_10.csv\n",
      "Saved Data_2_11.csv\n",
      "Saved Data_2_13.csv\n",
      "Saved Data_2_14.csv\n",
      "Saved Data_2_15.csv\n",
      "Saved Data_2_16.csv\n",
      "Saved Data_2_17.csv\n",
      "Saved Data_2_18.csv\n",
      "Saved Data_2_23.csv\n",
      "Saved Data_2_24.csv\n",
      "Saved Data_3_0.csv\n",
      "Saved Data_3_2.csv\n",
      "Saved Data_3_6.csv\n",
      "Saved Data_3_7.csv\n",
      "Saved Data_3_10.csv\n",
      "Saved Data_3_11.csv\n",
      "Saved Data_3_12.csv\n",
      "Saved Data_3_13.csv\n",
      "Saved Data_3_14.csv\n",
      "Saved Data_3_15.csv\n",
      "Saved Data_3_16.csv\n",
      "Saved Data_3_17.csv\n",
      "Saved Data_3_19.csv\n",
      "Saved Data_3_22.csv\n",
      "Saved Data_3_24.csv\n",
      "Saved Data_4_0.csv\n",
      "Saved Data_4_1.csv\n",
      "Saved Data_4_2.csv\n",
      "Saved Data_4_6.csv\n",
      "Saved Data_4_7.csv\n",
      "Saved Data_4_10.csv\n",
      "Saved Data_4_11.csv\n",
      "Saved Data_4_12.csv\n",
      "Saved Data_4_13.csv\n",
      "Saved Data_4_14.csv\n",
      "Saved Data_4_15.csv\n",
      "Saved Data_4_16.csv\n",
      "Saved Data_4_17.csv\n",
      "Saved Data_4_19.csv\n",
      "Saved Data_4_21.csv\n",
      "Saved Data_4_23.csv\n",
      "Saved Data_4_24.csv\n",
      "Saved Data_5_0.csv\n",
      "Saved Data_5_1.csv\n",
      "Saved Data_5_7.csv\n",
      "Saved Data_5_8.csv\n",
      "Saved Data_5_13.csv\n",
      "Saved Data_5_16.csv\n",
      "Saved Data_5_17.csv\n",
      "Saved Data_5_18.csv\n",
      "Saved Data_5_19.csv\n",
      "Saved Data_5_20.csv\n",
      "Saved Data_5_21.csv\n",
      "Saved Data_5_22.csv\n",
      "Saved Data_5_23.csv\n",
      "Saved Data_5_24.csv\n",
      "Saved Data_6_0.csv\n",
      "Saved Data_6_1.csv\n",
      "Saved Data_6_4.csv\n",
      "Saved Data_6_5.csv\n",
      "Saved Data_6_16.csv\n",
      "Saved Data_6_17.csv\n",
      "Saved Data_6_18.csv\n",
      "Saved Data_6_19.csv\n",
      "Saved Data_6_20.csv\n",
      "Saved Data_6_21.csv\n",
      "Saved Data_6_22.csv\n",
      "Saved Data_6_23.csv\n",
      "Saved Data_6_24.csv\n",
      "Saved Data_7_0.csv\n",
      "Saved Data_7_2.csv\n",
      "Saved Data_7_4.csv\n",
      "Saved Data_7_5.csv\n",
      "Saved Data_7_6.csv\n",
      "Saved Data_7_11.csv\n",
      "Saved Data_7_13.csv\n",
      "Saved Data_7_14.csv\n",
      "Saved Data_7_16.csv\n",
      "Saved Data_7_18.csv\n",
      "Saved Data_7_19.csv\n",
      "Saved Data_7_20.csv\n",
      "Saved Data_7_21.csv\n",
      "Saved Data_7_22.csv\n",
      "Saved Data_7_23.csv\n",
      "Saved Data_7_24.csv\n",
      "Saved Data_8_0.csv\n",
      "Saved Data_8_3.csv\n",
      "Saved Data_8_4.csv\n",
      "Saved Data_8_5.csv\n",
      "Saved Data_8_6.csv\n",
      "Saved Data_8_7.csv\n",
      "Saved Data_8_9.csv\n",
      "Saved Data_8_10.csv\n",
      "Saved Data_8_11.csv\n",
      "Saved Data_8_12.csv\n",
      "Saved Data_8_13.csv\n",
      "Saved Data_8_14.csv\n",
      "Saved Data_8_15.csv\n",
      "Saved Data_8_16.csv\n",
      "Saved Data_8_17.csv\n",
      "Saved Data_8_18.csv\n",
      "Saved Data_8_19.csv\n",
      "Saved Data_8_20.csv\n",
      "Saved Data_8_21.csv\n",
      "Saved Data_8_22.csv\n",
      "Saved Data_8_23.csv\n",
      "Saved Data_8_24.csv\n",
      "Saved Data_9_0.csv\n",
      "Saved Data_9_5.csv\n",
      "Saved Data_9_11.csv\n",
      "Saved Data_9_12.csv\n",
      "Saved Data_9_13.csv\n",
      "Saved Data_9_14.csv\n",
      "Saved Data_9_15.csv\n",
      "Saved Data_9_16.csv\n",
      "Saved Data_9_17.csv\n",
      "Saved Data_9_18.csv\n",
      "Saved Data_9_21.csv\n",
      "Saved Data_9_23.csv\n",
      "Saved Data_10_0.csv\n",
      "Saved Data_10_1.csv\n",
      "Saved Data_10_2.csv\n",
      "Saved Data_10_4.csv\n",
      "Saved Data_10_5.csv\n",
      "Saved Data_10_11.csv\n",
      "Saved Data_10_12.csv\n",
      "Saved Data_10_13.csv\n",
      "Saved Data_10_14.csv\n",
      "Saved Data_10_15.csv\n",
      "Saved Data_10_18.csv\n",
      "Saved Data_10_19.csv\n",
      "Saved Data_10_21.csv\n",
      "Saved Data_10_23.csv\n",
      "Saved Data_11_0.csv\n",
      "Saved Data_11_1.csv\n",
      "Saved Data_11_5.csv\n",
      "Saved Data_11_6.csv\n",
      "Saved Data_11_7.csv\n",
      "Saved Data_11_8.csv\n",
      "Saved Data_11_9.csv\n",
      "Saved Data_11_11.csv\n",
      "Saved Data_11_12.csv\n",
      "Saved Data_11_13.csv\n",
      "Saved Data_11_14.csv\n",
      "Saved Data_11_15.csv\n",
      "Saved Data_11_18.csv\n",
      "Saved Data_11_19.csv\n",
      "Saved Data_11_21.csv\n",
      "Saved Data_11_22.csv\n",
      "Saved Data_11_23.csv\n",
      "Saved Data_11_24.csv\n",
      "Saved Data_12_0.csv\n",
      "Saved Data_12_3.csv\n",
      "Saved Data_12_5.csv\n",
      "Saved Data_12_6.csv\n",
      "Saved Data_12_9.csv\n",
      "Saved Data_12_10.csv\n",
      "Saved Data_12_11.csv\n",
      "Saved Data_12_14.csv\n",
      "Saved Data_12_15.csv\n",
      "Saved Data_12_16.csv\n",
      "Saved Data_12_18.csv\n",
      "Saved Data_12_19.csv\n",
      "Saved Data_12_22.csv\n",
      "Saved Data_12_24.csv\n",
      "Saved Data_13_0.csv\n",
      "Saved Data_13_3.csv\n",
      "Saved Data_13_4.csv\n",
      "Saved Data_13_6.csv\n",
      "Saved Data_13_8.csv\n",
      "Saved Data_13_11.csv\n",
      "Saved Data_13_12.csv\n",
      "Saved Data_13_13.csv\n",
      "Saved Data_13_15.csv\n",
      "Saved Data_13_17.csv\n",
      "Saved Data_13_19.csv\n",
      "Saved Data_13_20.csv\n",
      "Saved Data_13_21.csv\n",
      "Saved Data_13_22.csv\n",
      "Saved Data_13_23.csv\n",
      "Saved Data_13_24.csv\n",
      "Saved Data_14_0.csv\n",
      "Saved Data_14_1.csv\n",
      "Saved Data_14_4.csv\n",
      "Saved Data_14_6.csv\n",
      "Saved Data_14_7.csv\n",
      "Saved Data_14_8.csv\n",
      "Saved Data_14_9.csv\n",
      "Saved Data_14_10.csv\n",
      "Saved Data_14_12.csv\n",
      "Saved Data_14_15.csv\n",
      "Saved Data_14_16.csv\n",
      "Saved Data_14_17.csv\n",
      "Saved Data_14_19.csv\n",
      "Saved Data_14_20.csv\n",
      "Saved Data_14_21.csv\n",
      "Saved Data_14_22.csv\n",
      "Saved Data_14_23.csv\n",
      "Saved Data_14_24.csv\n",
      "Saved Data_15_0.csv\n",
      "Saved Data_15_1.csv\n",
      "Saved Data_15_2.csv\n",
      "Saved Data_15_3.csv\n",
      "Saved Data_15_8.csv\n",
      "Saved Data_15_9.csv\n",
      "Saved Data_15_10.csv\n",
      "Saved Data_15_11.csv\n",
      "Saved Data_15_12.csv\n",
      "Saved Data_15_13.csv\n",
      "Saved Data_15_14.csv\n",
      "Saved Data_15_15.csv\n",
      "Saved Data_15_16.csv\n",
      "Saved Data_15_17.csv\n",
      "Saved Data_15_18.csv\n",
      "Saved Data_15_19.csv\n",
      "Saved Data_15_20.csv\n",
      "Saved Data_15_21.csv\n",
      "Saved Data_15_22.csv\n",
      "Saved Data_15_23.csv\n",
      "Saved Data_15_24.csv\n",
      "Saved Data_16_0.csv\n",
      "Saved Data_16_1.csv\n",
      "Saved Data_16_7.csv\n",
      "Saved Data_16_9.csv\n",
      "Saved Data_16_10.csv\n",
      "Saved Data_16_12.csv\n",
      "Saved Data_16_13.csv\n",
      "Saved Data_16_14.csv\n",
      "Saved Data_16_15.csv\n",
      "Saved Data_16_18.csv\n",
      "Saved Data_16_19.csv\n",
      "Saved Data_16_20.csv\n",
      "Saved Data_16_22.csv\n",
      "Saved Data_17_0.csv\n",
      "Saved Data_17_1.csv\n",
      "Saved Data_17_2.csv\n",
      "Saved Data_17_3.csv\n",
      "Saved Data_17_4.csv\n",
      "Saved Data_17_5.csv\n",
      "Saved Data_17_10.csv\n",
      "Saved Data_17_11.csv\n",
      "Saved Data_17_14.csv\n",
      "Saved Data_17_15.csv\n",
      "Saved Data_17_18.csv\n",
      "Saved Data_17_19.csv\n",
      "Saved Data_17_20.csv\n",
      "Saved Data_17_22.csv\n",
      "Saved Data_17_23.csv\n",
      "Saved Data_17_24.csv\n",
      "Saved Data_18_0.csv\n",
      "Saved Data_18_1.csv\n",
      "Saved Data_18_2.csv\n",
      "Saved Data_18_4.csv\n",
      "Saved Data_18_6.csv\n",
      "Saved Data_18_7.csv\n",
      "Saved Data_18_8.csv\n",
      "Saved Data_18_9.csv\n",
      "Saved Data_18_10.csv\n",
      "Saved Data_18_11.csv\n",
      "Saved Data_18_13.csv\n",
      "Saved Data_18_14.csv\n",
      "Saved Data_18_18.csv\n",
      "Saved Data_18_19.csv\n",
      "Saved Data_18_20.csv\n",
      "Saved Data_18_21.csv\n",
      "Saved Data_18_22.csv\n",
      "Saved Data_19_0.csv\n",
      "Saved Data_19_2.csv\n",
      "Saved Data_19_3.csv\n",
      "Saved Data_19_5.csv\n",
      "Saved Data_19_6.csv\n",
      "Saved Data_19_8.csv\n",
      "Saved Data_19_9.csv\n",
      "Saved Data_19_10.csv\n",
      "Saved Data_19_11.csv\n",
      "Saved Data_19_13.csv\n",
      "Saved Data_19_15.csv\n",
      "Saved Data_19_16.csv\n",
      "Saved Data_19_19.csv\n",
      "Saved Data_19_22.csv\n",
      "Saved Data_19_23.csv\n",
      "Saved Data_19_24.csv\n",
      "Saved Data_20_0.csv\n",
      "Saved Data_20_1.csv\n",
      "Saved Data_20_2.csv\n",
      "Saved Data_20_3.csv\n",
      "Saved Data_20_4.csv\n",
      "Saved Data_20_5.csv\n",
      "Saved Data_20_7.csv\n",
      "Saved Data_20_8.csv\n",
      "Saved Data_20_9.csv\n",
      "Saved Data_20_10.csv\n",
      "Saved Data_20_11.csv\n",
      "Saved Data_20_15.csv\n",
      "Saved Data_20_17.csv\n",
      "Saved Data_20_18.csv\n",
      "Saved Data_20_19.csv\n",
      "Saved Data_20_20.csv\n",
      "Saved Data_20_21.csv\n",
      "Saved Data_20_22.csv\n",
      "Saved Data_20_24.csv\n",
      "Saved Data_21_0.csv\n",
      "Saved Data_21_1.csv\n",
      "Saved Data_21_3.csv\n",
      "Saved Data_21_5.csv\n",
      "Saved Data_21_6.csv\n",
      "Saved Data_21_7.csv\n",
      "Saved Data_21_8.csv\n",
      "Saved Data_21_9.csv\n",
      "Saved Data_21_11.csv\n",
      "Saved Data_21_12.csv\n",
      "Saved Data_21_13.csv\n",
      "Saved Data_21_14.csv\n",
      "Saved Data_21_15.csv\n",
      "Saved Data_21_16.csv\n",
      "Saved Data_21_17.csv\n",
      "Saved Data_21_18.csv\n",
      "Saved Data_21_19.csv\n",
      "Saved Data_22_0.csv\n",
      "Saved Data_22_2.csv\n",
      "Saved Data_22_3.csv\n",
      "Saved Data_22_5.csv\n",
      "Saved Data_22_6.csv\n",
      "Saved Data_22_7.csv\n",
      "Saved Data_22_10.csv\n",
      "Saved Data_22_11.csv\n",
      "Saved Data_22_14.csv\n",
      "Saved Data_22_15.csv\n",
      "Saved Data_22_16.csv\n",
      "Saved Data_22_18.csv\n",
      "Saved Data_22_19.csv\n",
      "Saved Data_22_21.csv\n",
      "Saved Data_22_22.csv\n",
      "Saved Data_23_1.csv\n",
      "Saved Data_23_5.csv\n",
      "Saved Data_23_6.csv\n",
      "Saved Data_23_7.csv\n",
      "Saved Data_23_8.csv\n",
      "Saved Data_23_9.csv\n",
      "Saved Data_23_17.csv\n",
      "Saved Data_23_19.csv\n",
      "Saved Data_23_21.csv\n",
      "Saved Data_23_24.csv\n",
      "Saved Data_24_13.csv\n",
      "Saved Data_24_14.csv\n",
      "Saved Data_24_17.csv\n",
      "Saved Data_24_19.csv\n",
      "Saved Data_24_24.csv\n",
      "Filtered data saved to CSV files in folder 'filtered_data_files'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "df = pd.read_csv('data_combined.csv')\n",
    "\n",
    "# Define the range for column 'n'\n",
    "n = 24\n",
    "\n",
    "# Create a folder to store CSV files\n",
    "folder_name = 'filtered_data_files'\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Iterate over each row and filter data\n",
    "for row in range(0, n+1):\n",
    "    for column in range(0, n+1):\n",
    "        filtered_df = df[(df['Row'] == row) & (df['Column'] == column)]\n",
    "        if not filtered_df.empty:\n",
    "            # Create a filename based on Row and Column\n",
    "            filename = f'Data_{row}_{column}.csv'\n",
    "            filepath = os.path.join(folder_name, filename)\n",
    "            \n",
    "            # Save filtered data to CSV file\n",
    "            filtered_df.to_csv(filepath, index=False)\n",
    "            print(f\"Saved {filename}\")\n",
    "\n",
    "print(\"Filtered data saved to CSV files in folder 'filtered_data_files'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 3 distinct features nearest to 'Restaurant-FastFood' with their scores are: {'Restaurant-FastFood': 0.75, 'Fitness&Wellness-Gym': 0.7499242417846601, 'EducationalInstitute-School': 0.749723696020214}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('filtered_data_files/Data_0_0.csv')\n",
    "\n",
    "# Calculate the frequency of each Category-Subcategory\n",
    "frequency = df['Category-Subcategory'].value_counts()\n",
    "\n",
    "# Add a new column 'weights' based on the frequency\n",
    "df['weights'] = df['Category-Subcategory'].apply(lambda x: 1 / frequency[x])\n",
    "\n",
    "\n",
    "# Define the custom distance metric\n",
    "def custom_distance(point1, point2, w=0.5, r=0.5):\n",
    "    # point1 and point2 are arrays: [x, y, weight, rating]\n",
    "    distance = np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
    "    weight_factor = (point1[2] + point2[2]) / 2  # Average weight\n",
    "    rating_factor = (point1[3] + point2[3]) / 2  # Average rating\n",
    "    return ((w * r) + (weight_factor * distance * (1 / rating_factor)) / (r + (weight_factor * distance)))\n",
    "\n",
    "# Extract relevant columns for KNN\n",
    "data = df[['Latitude', 'Longitude', 'weights', 'FinalRating']].values  # Extract as numpy array\n",
    "\n",
    "# Fit the KNN model\n",
    "knn = NearestNeighbors(metric=custom_distance)\n",
    "knn.fit(data)\n",
    "\n",
    "# Function to find k distinct features based on the input feature\n",
    "def find_k_distinct_features(input_feature, k=5):\n",
    "    # Get the corresponding row(s) for the given input feature\n",
    "    query_indices = df.index[df['Category-Subcategory'] == input_feature].tolist()\n",
    "    if not query_indices:\n",
    "        return None\n",
    "    \n",
    "    query_point = data[query_indices[0]]  # Use the first matching row for query\n",
    "    distances, indices = knn.kneighbors([query_point], n_neighbors=len(df))\n",
    "    \n",
    "    # Get the features and distances of the nearest neighbors\n",
    "    nearest_features = df.iloc[indices[0]]['Category-Subcategory'].values\n",
    "    nearest_distances = distances[0]\n",
    "    \n",
    "    # Select k distinct features and their scores\n",
    "    distinct_features = {}\n",
    "    seen_features = set()\n",
    "    for feature, distance in zip(nearest_features, nearest_distances):\n",
    "        if feature not in seen_features:\n",
    "            distinct_features[feature] = 1 - distance\n",
    "            seen_features.add(feature)\n",
    "        if len(distinct_features) == k:\n",
    "            break\n",
    "    \n",
    "    return distinct_features\n",
    "\n",
    "# Example usage\n",
    "input_feature = 'Restaurant-FastFood'\n",
    "k = 3\n",
    "distinct_features_with_scores = find_k_distinct_features(input_feature, k)\n",
    "print(f\"The {k} distinct features nearest to '{input_feature}' with their scores are: {distinct_features_with_scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'final_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Define the custom distance metric\n",
    "def custom_distance(point1, point2, w=0.5, r=0.5):\n",
    "    distance = np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
    "    weight_factor = (point1[2] + point2[2]) / 2  # Average weight\n",
    "    rating_factor = (point1[3] + point2[3]) / 2  # Average rating\n",
    "    return ((w * r) + (weight_factor * distance * (1 / rating_factor)) / (r + (weight_factor * distance)))\n",
    "\n",
    "# Function to find k distinct features based on the input feature\n",
    "def find_k_distinct_features(df, knn, input_feature, k=5):\n",
    "    data = df[['Latitude', 'Longitude', 'weights', 'FinalRating']].values   # Extract relevant columns\n",
    "    query_indices = df.index[df['Category-Subcategory'] == input_feature].tolist()\n",
    "    if not query_indices:\n",
    "        return None\n",
    "    \n",
    "    query_point = data[query_indices[0]]  # Use the first matching row for query\n",
    "    distances, indices = knn.kneighbors([query_point], n_neighbors=len(df))\n",
    "    \n",
    "    # Get the features and distances of the nearest neighbors\n",
    "    nearest_features = df.iloc[indices[0]]['Category-Subcategory'].values\n",
    "    nearest_distances = distances[0]\n",
    "    \n",
    "    # Select k distinct features and their scores\n",
    "    distinct_features = {}\n",
    "    seen_features = set()\n",
    "    for feature, distance in zip(nearest_features, nearest_distances):\n",
    "        if feature not in seen_features:\n",
    "            distinct_features[feature] = 1 - distance  # Score is 1 - distance\n",
    "            seen_features.add(feature)\n",
    "        if len(distinct_features) == k:\n",
    "            break\n",
    "    \n",
    "    return distinct_features    \n",
    "\n",
    "# Load multiple datasets and find nearest features\n",
    "all_datas = glob.glob('filtered_data_files/Data*.csv')\n",
    "datasets = []\n",
    "\n",
    "# Iterate through each dataset\n",
    "for all_data in all_datas:\n",
    "    df = pd.read_csv(all_data)\n",
    "    datasets.append(df)\n",
    "\n",
    "# Assume ratings are extracted from each dataset\n",
    "ratings = [df['Population'].unique()[0] for df in datasets]  # List of rating values corresponding to each dataset\n",
    "input_feature = 'Restaurant-FastFood'\n",
    "k = 5\n",
    "\n",
    "results = []\n",
    "\n",
    "for dataset, rating in zip(datasets, ratings):\n",
    "    # Extract relevant columns for KNN\n",
    "    data = dataset[['Latitude', 'Longitude', 'weights', 'FinalRating']].values \n",
    "    \n",
    "    # Fit the KNN model\n",
    "    knn = NearestNeighbors(metric=custom_distance)\n",
    "    knn.fit(data)\n",
    "    \n",
    "    # Find k distinct features\n",
    "    distinct_features = find_k_distinct_features(dataset, knn, input_feature, k)\n",
    "    \n",
    "    if distinct_features:\n",
    "        for feature, score in distinct_features.items():\n",
    "            results.append({'Category-Subcategory': feature, 'score': score, 'Population': rating})\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('final_results.csv', index=False)\n",
    "\n",
    "print(f\"Results saved to 'final_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('final_results.csv')\n",
    "\n",
    "# Filter rows where col1 equals 'k'\n",
    "df_filtered = df[df['Category-Subcategory'] != 'Restaurant-FastFood']\n",
    "\n",
    "# Save the filtered DataFrame back to a CSV file\n",
    "df_filtered.to_csv('final_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Category-Subcategory  importance\n",
      "18       Fitness&Wellness-YogaStudio    0.220170\n",
      "20    GovernmentBuilding-FireStation    0.147327\n",
      "5   EducationalInstitute-MusicSchool    0.063186\n",
      "6        EducationalInstitute-School    0.058920\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the data from CSV file\n",
    "df = pd.read_csv('final_results.csv')\n",
    "\n",
    "# One-hot encode the 'Category-Subcategory' column\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_features = encoder.fit_transform(df[['Category-Subcategory']])\n",
    "encoded_feature_names = encoder.get_feature_names_out(['Category-Subcategory'])\n",
    "df_encoded = pd.DataFrame(encoded_features, columns=encoded_feature_names)\n",
    "\n",
    "# Combine the encoded features with the original dataframe\n",
    "df_combined = pd.concat([df.drop(columns=['Category-Subcategory']), df_encoded], axis=1)\n",
    "\n",
    "# Ensure 'Population' is included in the feature set\n",
    "# Separate features and target\n",
    "X = df_combined.drop(columns=[\"score\"])\n",
    "y = df_combined[\"score\"]\n",
    "\n",
    "# Train a Random Forest Regressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": features,\n",
    "    \"importance\": feature_importances\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "# Filter to get the top 4 features from 'Category-Subcategory'\n",
    "top_features = importance_df[importance_df['feature'].str.startswith('Category-Subcategory')].head(4)\n",
    "\n",
    "# Remove 'Category-Subcategory_' prefix\n",
    "top_features['Category-Subcategory'] = top_features['feature'].str.replace('Category-Subcategory_', '')\n",
    "\n",
    "# Select relevant columns and display\n",
    "top_features = top_features[['Category-Subcategory', 'importance']]\n",
    "print(top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Latitude  Longitude  Row  Column  Population  FinalRating  \\\n",
      "0    30.714827  76.738702   10       1      8454.6     2.750000   \n",
      "1    30.714827  76.738702   10       1      8454.6     2.750000   \n",
      "2    30.714827  76.738702   10       1      8454.6     2.750000   \n",
      "3    30.741681  76.747399   21       9      9005.0     3.920000   \n",
      "4    30.741681  76.747399   21       9      9005.0     3.920000   \n",
      "..         ...        ...  ...     ...         ...          ...   \n",
      "819  30.741367  76.766978   24      24      2495.6     3.936000   \n",
      "820  30.739010  76.761064   20      21      7887.6     3.500000   \n",
      "821  30.740178  76.757908   20      18      8137.4     4.500000   \n",
      "822  30.742323  76.758953   21      19      8039.4     4.307692   \n",
      "823  30.732280  76.770713   24      24      2495.6     2.750000   \n",
      "\n",
      "                 Category-Subcategory   weights  importance  \n",
      "0      GovernmentBuilding-FireStation  0.047619    0.147327  \n",
      "1      GovernmentBuilding-FireStation  0.047619    0.147327  \n",
      "2      GovernmentBuilding-FireStation  0.047619    0.147327  \n",
      "3      GovernmentBuilding-FireStation  0.047619    0.147327  \n",
      "4      GovernmentBuilding-FireStation  0.047619    0.147327  \n",
      "..                                ...       ...         ...  \n",
      "819       EducationalInstitute-School  0.003571    0.058920  \n",
      "820  EducationalInstitute-MusicSchool  0.003774    0.063186  \n",
      "821  EducationalInstitute-MusicSchool  0.003774    0.063186  \n",
      "822  EducationalInstitute-MusicSchool  0.003774    0.063186  \n",
      "823  EducationalInstitute-MusicSchool  0.003774    0.063186  \n",
      "\n",
      "[824 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example of top_features DataFrame, replace this with the actual top_features DataFrame you have\n",
    "\n",
    "\n",
    "# Load the data_combined.csv file\n",
    "data_combined = pd.read_csv('data_combined.csv')\n",
    "\n",
    "# Extract Latitude and Longitude for the rows containing Category-Subcategory from top_features\n",
    "top_categories = top_features['Category-Subcategory']\n",
    "filtered_data = data_combined[data_combined['Category-Subcategory'].isin(top_categories)]\n",
    "\n",
    "# Merge with top_features to include importance\n",
    "final_data = filtered_data.merge(top_features, on='Category-Subcategory')\n",
    "\n",
    "# Save the result to model.csv\n",
    "final_data.to_csv('model.csv', index=False)\n",
    "\n",
    "print(final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Latitude  Longitude  Row  Column  Population  FinalRating  \\\n",
      "0    30.714827  76.738702   10       1      8454.6     2.750000   \n",
      "1    30.714827  76.738702   10       1      8454.6     2.750000   \n",
      "2    30.714827  76.738702   10       1      8454.6     2.750000   \n",
      "3    30.741681  76.747399   21       9      9005.0     3.920000   \n",
      "4    30.741681  76.747399   21       9      9005.0     3.920000   \n",
      "..         ...        ...  ...     ...         ...          ...   \n",
      "819  30.614551  76.878546   24      24      2495.6     2.750000   \n",
      "820  30.739010  76.761064   20      21      7887.6     3.500000   \n",
      "821  30.740178  76.757908   20      18      8137.4     4.500000   \n",
      "822  30.742323  76.758953   21      19      8039.4     4.307692   \n",
      "823  30.732280  76.770713   24      24      2495.6     2.750000   \n",
      "\n",
      "                 Category-Subcategory   weights  importance  predictions  \n",
      "0      GovernmentBuilding-FireStation  0.047619    0.147327     0.147327  \n",
      "1      GovernmentBuilding-FireStation  0.047619    0.147327     0.147327  \n",
      "2      GovernmentBuilding-FireStation  0.047619    0.147327     0.147327  \n",
      "3      GovernmentBuilding-FireStation  0.047619    0.147327     0.147327  \n",
      "4      GovernmentBuilding-FireStation  0.047619    0.147327     0.147327  \n",
      "..                                ...       ...         ...          ...  \n",
      "819  EducationalInstitute-MusicSchool  0.003774    0.063186     0.063186  \n",
      "820  EducationalInstitute-MusicSchool  0.003774    0.063186     0.063186  \n",
      "821  EducationalInstitute-MusicSchool  0.003774    0.063186     0.063186  \n",
      "822  EducationalInstitute-MusicSchool  0.003774    0.063186     0.063186  \n",
      "823  EducationalInstitute-MusicSchool  0.003774    0.063186     0.063186  \n",
      "\n",
      "[824 rows x 10 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_13768\\1859564829.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  category_data['predictions'] = predictions\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_13768\\1859564829.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  category_data['predictions'] = predictions\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_13768\\1859564829.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  category_data['predictions'] = predictions\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_13768\\1859564829.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  category_data['predictions'] = predictions\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load the model.csv file containing Latitude, Longitude, and importance\n",
    "model_data = pd.read_csv('model.csv')\n",
    "\n",
    "# List of Category-Subcategory values\n",
    "categories = model_data['Category-Subcategory'].unique()\n",
    "\n",
    "# Dictionary to store predictions for each Category-Subcategory\n",
    "predictions_dict = {}\n",
    "\n",
    "# Train a separate model for each Category-Subcategory\n",
    "for category in categories:\n",
    "    # Filter data for the current Category-Subcategory\n",
    "    category_data = model_data[model_data['Category-Subcategory'] == category]\n",
    "    \n",
    "    # Prepare features (Latitude, Longitude) and target (importance)\n",
    "    X = category_data[['Latitude', 'Longitude']]\n",
    "    y = category_data['importance']\n",
    "    \n",
    "    # Train a RandomForestRegressor model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Predict importance for the current Category-Subcategory\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    # Store predictions in the dictionary\n",
    "    predictions_dict[category] = predictions\n",
    "\n",
    "# Create a DataFrame to hold all predictions\n",
    "predictions_df = pd.DataFrame()\n",
    "\n",
    "for category, predictions in predictions_dict.items():\n",
    "    category_data = model_data[model_data['Category-Subcategory'] == category]\n",
    "    category_data['predictions'] = predictions\n",
    "    predictions_df = pd.concat([predictions_df, category_data], ignore_index=True)\n",
    "\n",
    "# Save predictions to a new CSV file\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n",
    "\n",
    "print(predictions_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Latitude: 30.713844\n",
      "Best Longitude: 76.753389\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('model.csv')\n",
    "\n",
    "# Group by Latitude and Longitude and calculate the weighted importance\n",
    "df['WeightedImportance'] = df['FinalRating'] * df['importance']\n",
    "grouped_df = df.groupby(['Latitude', 'Longitude']).agg({'WeightedImportance': 'sum'}).reset_index()\n",
    "\n",
    "# Find the Latitude and Longitude with the highest weighted importance\n",
    "best_location = grouped_df.loc[grouped_df['WeightedImportance'].idxmax()]\n",
    "\n",
    "print(f\"Best Latitude: {best_location['Latitude']}\")\n",
    "print(f\"Best Longitude: {best_location['Longitude']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Latitude: 30.862174\n",
      "Best Longitude: 76.662754\n",
      "Maximum Weighted Importance: 402.1613451092763\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('model.csv')\n",
    "\n",
    "# Calculate Weighted Importance for each row\n",
    "df['WeightedImportance'] = df['FinalRating'] * df['importance']\n",
    "\n",
    "# Group by Latitude and Longitude and sum the weighted importance\n",
    "grouped_df = df.groupby(['Latitude', 'Longitude']).agg({'WeightedImportance': 'sum'}).reset_index()\n",
    "\n",
    "# DP table initialization\n",
    "n = len(grouped_df)\n",
    "dp = [0] * (n + 1)\n",
    "\n",
    "# Compute the DP table\n",
    "for i in range(1, n + 1):\n",
    "    dp[i] = max(dp[i-1], dp[i-1] + grouped_df.loc[i-1, 'WeightedImportance'])\n",
    "\n",
    "# Find the best location\n",
    "max_importance = max(dp)\n",
    "best_index = dp.index(max_importance)\n",
    "\n",
    "best_location = grouped_df.loc[best_index - 1]\n",
    "\n",
    "print(f\"Best Latitude: {best_location['Latitude']}\")\n",
    "print(f\"Best Longitude: {best_location['Longitude']}\")\n",
    "print(f\"Maximum Weighted Importance: {max_importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Latitude  Longitude  Row  Column  Population  FinalRating  \\\n",
      "0    30.714827  76.738702   10       1      8454.6     2.750000   \n",
      "1    30.714827  76.738702   10       1      8454.6     2.750000   \n",
      "2    30.714827  76.738702   10       1      8454.6     2.750000   \n",
      "3    30.741681  76.747399   21       9      9005.0     3.920000   \n",
      "4    30.741681  76.747399   21       9      9005.0     3.920000   \n",
      "..         ...        ...  ...     ...         ...          ...   \n",
      "819  30.741367  76.766978   24      24      2495.6     3.936000   \n",
      "820  30.739010  76.761064   20      21      7887.6     3.500000   \n",
      "821  30.740178  76.757908   20      18      8137.4     4.500000   \n",
      "822  30.742323  76.758953   21      19      8039.4     4.307692   \n",
      "823  30.732280  76.770713   24      24      2495.6     2.750000   \n",
      "\n",
      "                 Category-Subcategory   weights  importance  \\\n",
      "0      GovernmentBuilding-FireStation  0.047619    0.147327   \n",
      "1      GovernmentBuilding-FireStation  0.047619    0.147327   \n",
      "2      GovernmentBuilding-FireStation  0.047619    0.147327   \n",
      "3      GovernmentBuilding-FireStation  0.047619    0.147327   \n",
      "4      GovernmentBuilding-FireStation  0.047619    0.147327   \n",
      "..                                ...       ...         ...   \n",
      "819       EducationalInstitute-School  0.003571    0.058920   \n",
      "820  EducationalInstitute-MusicSchool  0.003774    0.063186   \n",
      "821  EducationalInstitute-MusicSchool  0.003774    0.063186   \n",
      "822  EducationalInstitute-MusicSchool  0.003774    0.063186   \n",
      "823  EducationalInstitute-MusicSchool  0.003774    0.063186   \n",
      "\n",
      "     WeightedImportance  \n",
      "0              0.405149  \n",
      "1              0.405149  \n",
      "2              0.405149  \n",
      "3              0.577521  \n",
      "4              0.577521  \n",
      "..                  ...  \n",
      "819            0.231908  \n",
      "820            0.221152  \n",
      "821            0.284338  \n",
      "822            0.272187  \n",
      "823            0.173762  \n",
      "\n",
      "[814 rows x 10 columns]\n",
      "Best Latitude: 30.75098\n",
      "Best Longitude: 76.756848\n",
      "Maximum Weighted Importance: 400.0331732967293\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('model.csv')\n",
    "\n",
    "# Calculate Weighted Importance for each row\n",
    "df['WeightedImportance'] = df['FinalRating'] * df['importance']\n",
    "\n",
    "# Filter the dataset to ensure the values fall within the specified range\n",
    "latitude_min = 30.60805029690371\n",
    "latitude_max = 30.790863413283007\n",
    "longitude_min = 76.70726499899425\n",
    "longitude_max = 76.7948349520892\n",
    "\n",
    "filtered_df = df[(df['Latitude'] >= latitude_min) & (df['Latitude'] <= latitude_max) &\n",
    "                 (df['Longitude'] >= longitude_min) & (df['Longitude'] <= longitude_max)]\n",
    "print(filtered_df)\n",
    "# Group by Latitude and Longitude and sum the weighted importance\n",
    "grouped_df = filtered_df.groupby(['Latitude', 'Longitude']).agg({'WeightedImportance': 'sum'}).reset_index()\n",
    "\n",
    "# DP table initialization\n",
    "n = len(grouped_df)\n",
    "dp = [0] * (n + 1)\n",
    "\n",
    "# Store corresponding latitude and longitude\n",
    "locations = [(0, 0)] * (n + 1)\n",
    "\n",
    "# Compute the DP table\n",
    "for i in range(1, n + 1):\n",
    "    if dp[i-1] > dp[i-1] + grouped_df.loc[i-1, 'WeightedImportance']:\n",
    "        dp[i] = dp[i-1]\n",
    "        locations[i] = locations[i-1]\n",
    "    else:\n",
    "        dp[i] = dp[i-1] + grouped_df.loc[i-1, 'WeightedImportance']\n",
    "        locations[i] = (grouped_df.loc[i-1, 'Latitude'], grouped_df.loc[i-1, 'Longitude'])\n",
    "\n",
    "# Find the best location\n",
    "max_importance = max(dp)\n",
    "best_index = dp.index(max_importance)\n",
    "best_location = locations[best_index]\n",
    "\n",
    "print(f\"Best Latitude: {best_location[0]}\")\n",
    "print(f\"Best Longitude: {best_location[1]}\")\n",
    "print(f\"Maximum Weighted Importance: {max_importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Latitude: 30.75098\n",
      "Best Longitude: 76.756848\n",
      "Maximum Weighted Importance: 400.0331732967293\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('model.csv')\n",
    "\n",
    "# Calculate Weighted Importance for each row\n",
    "df['WeightedImportance'] = df['FinalRating'] * df['importance']\n",
    "\n",
    "# Filter the dataset to ensure the values fall within the specified range\n",
    "latitude_min = 30.60805029690371\n",
    "latitude_max = 30.790863413283007\n",
    "longitude_min = 76.70726499899425\n",
    "longitude_max = 76.7948349520892\n",
    "\n",
    "filtered_df = df[(df['Latitude'] >= latitude_min) & (df['Latitude'] <= latitude_max) &\n",
    "                 (df['Longitude'] >= longitude_min) & (df['Longitude'] <= longitude_max)]\n",
    "\n",
    "# Group by Latitude and Longitude and sum the weighted importance\n",
    "grouped_df = filtered_df.groupby(['Latitude', 'Longitude']).agg({'WeightedImportance': 'sum'}).reset_index()\n",
    "\n",
    "# DP table initialization\n",
    "n = len(grouped_df)\n",
    "dp = [0] * (n + 1)\n",
    "\n",
    "# Store corresponding latitude and longitude\n",
    "locations = [(0, 0)] * (n + 1)\n",
    "\n",
    "# Compute the DP table\n",
    "for i in range(1, n + 1):\n",
    "    if dp[i-1] > dp[i-1] + grouped_df.loc[i-1, 'WeightedImportance']:\n",
    "        dp[i] = dp[i-1]\n",
    "        locations[i] = locations[i-1]\n",
    "    else:\n",
    "        dp[i] = dp[i-1] + grouped_df.loc[i-1, 'WeightedImportance']\n",
    "        locations[i] = (grouped_df.loc[i-1, 'Latitude'], grouped_df.loc[i-1, 'Longitude'])\n",
    "\n",
    "# Find the best location\n",
    "max_importance = max(dp)\n",
    "best_index = dp.index(max_importance)\n",
    "best_location = locations[best_index]\n",
    "\n",
    "print(f\"Best Latitude: {best_location[0]}\")\n",
    "print(f\"Best Longitude: {best_location[1]}\")\n",
    "print(f\"Maximum Weighted Importance: {max_importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
